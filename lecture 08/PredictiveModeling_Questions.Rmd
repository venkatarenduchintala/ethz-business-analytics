---
title: "Practical Session 2: Predictive Modeling"
output: html_notebook
---

# Guidelines

This sheet presents a collection of exercises as part of the send practical session. It is designed as 'do-it-yourself' process where you can ask questions interactively upon request. 

Our recommendations:

* Ask questions! Reach out to your neighbors or the teaching assistants. As a first step, it is also wise to consult the official help pages or a websearch. 

* Follow up with the background materials where necessary. We compiled slide decks concerning several focus areas (ensembles, regularization, etc.). These should cover all relevant materials; however, you might find it more effective either to skim the materials or search for background literature online. 

* This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. We also uploaded a short tutorial on R Markdown. 

# Setup 

## Packages

R is shipped only with a limited core functionality, while most of its strength comes from additional packages that can be installed. Let's say you later need a package *ggplot2*, then you need to install it the first time. For this purpose, simply click on *Tools* in the menu, followed by *Install packages ...*, enter the name and then dlick *Install*. 

After installing the package, you can load it via. If you don't have it installed, you will receive an error message telling you so. 

```{r}
library(ggplot2)
```

As a general recommendation, I strongly suggest to collect all calls to *library(...)* at the beginning of your document. Thereby, you have all of them at glance, you circumvent to load packages multiple times (in rare cases this can cause side-effects) and, when porting your script to a different PC, you can easily see which packages to install. 

## Data loading

Throughout this session, we will work predominantly with one dataset called *GermanCredit*. This contains various personal variables from people applying for credit loans and an additional indicator whether there was a default. Our task will be to understand the loan application process and to make predictions to the bank on who will be likely to experience default. This will help the decision-makers in reducing money at risk and help their operations. 
The data is loaded via:

```{r}
library(caret)
data(GermanCredit)
```

Afterwards, you can access the data simply by typing *GermanCredit*. Here is an example:

```{r}
View(GermanCredit)
```


## Exploratory data analysis

As a first step, make sure that the data was correctly loaded and how its shape is. For this reason, check the size of the dataset via *dim(...)* 

```{r}
dim(GermanCredit)
```

In the following, you will find additional information that might help you along the way. ##The dataset contains the following column names:

```{r}
colnames(GermanCredit)
```

The default is stored in a column named *Class*. Let's look into that one more closely. *head(...)* prints the first rows of it and *str(...)* returns the data type. As you see, it is stored as *factor* which means that it is a categorical variable with exactly two options. This will later be necessary for classification tasks. 

```{r}
head(GermanCredit$Class)
str(GermanCredit$Class)

head(GermandCredit)
```

Finally, here is breakdown of frequencies for each class:

```{r}
table(GermanCredit$Class)
```

First task: what is the ratio of good credits?

```{r} 
numberOfExistingCredits <- GermanCredit$NumberExistingCredits
print(numberOfExistingCredits)
```


# Predictive modeling

This section is designed to introduce the basic functions for predictive modeling in R. 

* Level: beginners

* Accompanying materials: 5_PredictiveModeling.pdf (please go through this material briefly before running the exercises)

* Goals: after this section, you will be able to estimate predictive models, namely, the linear model and a k-nearest neighbor classifier. You will also be able to split your data in a training and a test set. 

## Training vs. test set

Data in predictive modeling is usally divided into a training and test set. Now perform such a split for *GermanCredit*. Use the help pages to look up the meaning of the argument *p*.


```{r}
library(caret)

set.seed(0) # this is added so that the random number generator is alwas giving the same split, which makes it easier to compare results for now.

inTrain <- createDataPartition(GermanCredit$Class, p = 0.8, list = FALSE)

training_set <- GermanCredit[inTrain, ]
test_set <- GermanCredit[-inTrain, ]
```


## Linear model

### Regression 

Let's start with a simple predictive model. For now, we want to predict the age of a customer based on the credit amount and the duration. This could facilitate a marketing manager in providing a tailored service. What is the root mean squared error on the test set? 

```{r}

```

Is this prediction better than random (e.g. a model without predictors and only an intercept)? Hint: you can estimate such a model via *lm(y ~ 1)*.
 
```{r}

```

Now interpret the model with amount and duration. Which coefficients are significant? What could be reasons why the model has a lower predictive error than the intercept even though none of the variables are signifcant?

```{r}

```



## k-nearest neighbor

Let's fit a kNN to the data. For simplicity, you can use a smaller dataset and restrict your kNN model to three columns (*Age*, *Duration*, and *Amount*). What would be the score for a person with age 37, a duration of 2 months, and an requested amount of 3000. 

```{r}

```

Is this prediction sensitive to your choice of k? Why not? Hint: think about the relative frequencies of the different labels.

# Ensemble learning

This section is designed to review the decision tree algorithm, as well as ensemble learning. 

* Level: intermediate

* Accompanying materials: 8_EnsembleLearning.pdf (you will need to follow the materials along your way)

* Goals: after this section, you will be able to create decision trees (including pruning) and estimate model ensembles (including the random forest). 

As a start, run the following chunk as the decision tree algorithm is sometimes sensitive to how random splits are generated. 

```{r}
set.seed(123)
```


## Decision tree

Build a decision tree using all variables in the training aset and visualize it. Hint: you do not need to type all variables individually, rather there is an abbreviation for that (which you encountered earlier).

```{r}

```

Your previous decision tree should look quite "messy". Now let's try to prune it and repeat to visualize it.

```{r}

```

What is the accuracy of the initial decision tree and its pruned counterpart on the test set? 

```{r}

```

You might have noticed that the interfaces to the decision tree and the random forest vary slightly. There is a unified interface to all prediction models (the *caret* package) and we will learn about in the next lab session.

## Random forest

Now estimate the accuracy of the random forest on the test set. 
```{r}

```

What are relevant variables for forecasting? Now look into the variable importance metrics.

```{r}

```

## Boosted linear model

Now estimate the accuracy of a bosted generalized linear model on the test set. Hint: you will need again a call in the form of *predict(model, newdata, type = "class")*.

```{r}

```

## AdaBoost

Now estimate the accuracy of AdaBoost on the test set. Hint: you will need again a call in the form of *predict(model, newdata, type = "class")*.

```{r}

```

## Wrap-up

Which model has performed best? What are potential reasons that the predictive accuracy is not higher (e.g. 90 percent)? 


Which models perform better than a majority vote? Be careful that you take the imbalanced class frequencies into account. Note that you cannot rely on the relative frequency of *Good* that we computed above, since we need to count our matches in the test set (which could have slighly different *Good* credits than in the overall population). Hint: the command *rep(value, frequency)* can help you in creating vectors with repetitive values.

```{r}
# create dummy predictor that always votes for "Good"
pred <- rep("Good", nrow(GermanCredit[-inTrain, ]))

```

What would be the financial benefit from using the random forest over a majority vote within a bank? What is the average return on a customer level (i.e. the overall return divided by the number of loan applications)?

```{r}

```

# Regularization

This section is designed to review regularization methods. 

* Level: advanced

* Accompanying materials: 6_Regularization.pdf (you will need to follow the materials along your way)

* Goals: after this section, you will be able to create a LASSO model and ridge regression. 

# Data preparation

The *glmnet* package requires the input to be in a different format; i.e. it expects separate matrices for x and y, instead of the previous formula notation. Now convert the *GermanCredit* dataset into the desired format. For simplicity, we now predict the credit amount (column: *Amount*) based on *Age* and *Duration* for targeted marketing. 

```{r}

```

Note: *glmnet* is a bit more tricky to use when working with classifcation tasks but we learn a very smooth way in the next lab.

## Ridge regression

Estimate a ridge gression and plot how the coefficients vary with *lambda*.

```{r}

```

## LASSO

Estimate a LASSO and again plot the coefficients

```{r}

```

Now use the routine *cv.glmnet* to tune the *lambda* inside the LASSO and look at the final coefficients.

```{r}

```


# Trend lines

This section is designed to review visualization techniques for plotting trend lines. Usually this is a step that one will undertake at the beginning of the analysis; however, the session postponed it as you might need to recap the visualization techniques from the previous slide deck (file: 3_AdvancedPlotting.pdf) 

* Level: advanced

* Accompanying materials: 7_NonLinearRegression.pdf (you will need to follow the materials along your way)

* Goals: after this section, you will be able to add trend lines to your plots.

## Point plot

The following snippet will generate a scatter plot, showing the distribution between age and credit amount. 

```{r}
library(ggplot2)

ggplot(GermanCredit, aes(x = Age, y = Amount)) +
  geom_point(size = 0.5) +
  geom_jitter(size = 0.5)
```

A firm might want to optimize its marketing strategy and, hence, a question could be whether different age groups should receive tailored ads. Specifically, whether young people have a tendency to apply for micro credits. 

## Linear trend line

Extend the previous plot to incorporate a linear trend line. 

```{r}

```

In your answer, will hardly see any age-specific variations of the credit amount. But is this true? 


## Generalized additive model

Now add instead a trend line that is based on a generalized additive model. 

```{r}

```

The GAM is supposed to include non-linear relationships, but we hardly find evidence here. What are potential reasons for this? Hint: specifically compare the idea of the GAM vs. LOESS. You can also ponder about different ways data might entail a trend and when GAM vs. LOESS will recognize it correctly. 

## LOESS trend line

Now experiment with the LOESS trend line. How is your choice of the *scale* parameter affecting the fit/smoothness of the trend line.  

```{r}


```

As expected, we now see a slight decline in the amount for young people. But would you act on this? Hint: double-check the confidence intervals and interpret them.

What are potential pros/cons of using GAM vs. LOeSS? Specifically think about the quality of the fit, sensitivity to noise and smoothness.

We can now come up with statistical rigor to check our previous assumption whether young people tend to ask for micro credits. Think about how you could do this with a step-wise non-linear regression. 

Alternatively, one could argue to use a simple hypothesis tests. What are situations when this test will not be appropriate? Hint: multivariate data. 

